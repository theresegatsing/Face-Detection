{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f625462d-8d06-43d1-91a8-10a0514f8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import os\n",
    "from PIL import Image\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "567237b3-3ab1-4ff3-8fa6-cfbbda985afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"dataset\", img_size=(64, 64)):\n",
    "    sets = [\"train\", \"test\"]\n",
    "    classes = [\"no_face\", \"face\"]\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for s in sets:\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for label, cls in enumerate(classes):\n",
    "            class_dir = os.path.join(data_dir, s, cls)\n",
    "\n",
    "            for file in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, file)\n",
    "\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert(\"RGB\")\n",
    "                    img = img.resize(img_size)\n",
    "                    X.append(np.array(img))\n",
    "                    y.append(label)\n",
    "                except:\n",
    "                    continue  # skip corrupted images\n",
    "\n",
    "        data[s] = (np.array(X), np.array(y))\n",
    "\n",
    "    train_x_orig, train_y_orig = data[\"train\"]\n",
    "    test_x_orig, test_y_orig = data[\"test\"]\n",
    "    \n",
    "    # Flatten images\n",
    "    train_x = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "    test_x  = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "    \n",
    "    # Normalize\n",
    "    train_x = train_x.astype(np.float32) / 255.0\n",
    "    test_x  = test_x.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Labels: reshape ONLY to (1, m)\n",
    "    train_y = train_y_orig.reshape(1, -1)\n",
    "    test_y  = test_y_orig.reshape(1, -1)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, np.array(classes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c8484b2-e180-4ef0-9328-b50657d9c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07b92fd8-a2d7-4cd7-b282-45f0ce9b7578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (224,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Testing the load_data function to ensure everything is correctly extracted\u001b[39;00m\n\u001b[0;32m      2\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m194\u001b[39m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(train_x[index])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(train_y[index]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. It\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m classes[train_y[index]] \u001b[38;5;241m+\u001b[39m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m picture.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:3592\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   3570\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[0;32m   3571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimshow\u001b[39m(\n\u001b[0;32m   3572\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3590\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3591\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[1;32m-> 3592\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m gca()\u001b[38;5;241m.\u001b[39mimshow(\n\u001b[0;32m   3593\u001b[0m         X,\n\u001b[0;32m   3594\u001b[0m         cmap\u001b[38;5;241m=\u001b[39mcmap,\n\u001b[0;32m   3595\u001b[0m         norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[0;32m   3596\u001b[0m         aspect\u001b[38;5;241m=\u001b[39maspect,\n\u001b[0;32m   3597\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[0;32m   3598\u001b[0m         alpha\u001b[38;5;241m=\u001b[39malpha,\n\u001b[0;32m   3599\u001b[0m         vmin\u001b[38;5;241m=\u001b[39mvmin,\n\u001b[0;32m   3600\u001b[0m         vmax\u001b[38;5;241m=\u001b[39mvmax,\n\u001b[0;32m   3601\u001b[0m         colorizer\u001b[38;5;241m=\u001b[39mcolorizer,\n\u001b[0;32m   3602\u001b[0m         origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[0;32m   3603\u001b[0m         extent\u001b[38;5;241m=\u001b[39mextent,\n\u001b[0;32m   3604\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m   3605\u001b[0m         filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[0;32m   3606\u001b[0m         filterrad\u001b[38;5;241m=\u001b[39mfilterrad,\n\u001b[0;32m   3607\u001b[0m         resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m   3608\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   3609\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3610\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3611\u001b[0m     )\n\u001b[0;32m   3612\u001b[0m     sci(__ret)\n\u001b[0;32m   3613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:1521\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\n\u001b[0;32m   1522\u001b[0m             ax,\n\u001b[0;32m   1523\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(cbook\u001b[38;5;241m.\u001b[39msanitize_sequence, args),\n\u001b[0;32m   1524\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: cbook\u001b[38;5;241m.\u001b[39msanitize_sequence(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m   1526\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1527\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1528\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:5945\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5943\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m-> 5945\u001b[0m im\u001b[38;5;241m.\u001b[39mset_data(X)\n\u001b[0;32m   5946\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5948\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\image.py:675\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    674\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_image_array(A)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\image.py:643\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m    641\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    649\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (224,) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Testing the load_data function to ensure everything is correctly extracted\n",
    "index = 194\n",
    "plt.imshow(train_x[index])\n",
    "print (\"y = \" + str(train_y[index]) + \". It's a \" + classes[train_y[index]] +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d938bf06-4d13-41a5-8b9c-2f1bb905ad78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 224\n",
      "Number of testing examples: 287\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (224, 64, 64, 3)\n",
      "train_y shape: (224,)\n",
      "test_x_orig shape: (287, 64, 64, 3)\n",
      "test_y shape: (287,)\n"
     ]
    }
   ],
   "source": [
    "#Exploring dataset to know the total size and other parameters\n",
    "\n",
    "train = train_x_orig.shape[0]\n",
    "test = test_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "\n",
    "print (\"Number of training examples: \" + str(train))\n",
    "print (\"Number of testing examples: \" + str(test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1d6f098-1115-4776-93ba-263da774e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [64*64*3, 64, 32, 1]\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) #number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters_deep(layer_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "260a2912-123a-46bf-9c4f-20fbc2f0ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.dot(W,A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4917395-22a0-4db9-86c6-8a76e4abe610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 [[-0.00450019 -0.00959911 -0.00836246 ... -0.00551261 -0.00599057\n",
      "  -0.00764693]\n",
      " [-0.00352297 -0.00489753 -0.00658281 ... -0.00412288 -0.00497707\n",
      "  -0.00553568]\n",
      " [ 0.00046374  0.00256457  0.00220269 ...  0.00112307 -0.00048832\n",
      "   0.00065474]\n",
      " ...\n",
      " [ 0.00019071 -0.00189658 -0.00299062 ...  0.00106858  0.00249432\n",
      "   0.00019529]\n",
      " [-0.00184841 -0.00303199 -0.00165671 ... -0.00107128 -0.00158586\n",
      "  -0.00020609]\n",
      " [ 0.00220308  0.00487628  0.00637429 ...  0.0035784   0.00261202\n",
      "   0.00259312]]\n",
      "cache1 (array([[0.00226067, 0.00390619, 0.00392157, ..., 0.00219915, 0.00252211,\n",
      "        0.00387543],\n",
      "       [0.00215302, 0.00390619, 0.00392157, ..., 0.00196847, 0.00126105,\n",
      "        0.00387543],\n",
      "       [0.00284506, 0.00390619, 0.00392157, ..., 0.00152249, 0.00161476,\n",
      "        0.00384468],\n",
      "       ...,\n",
      "       [0.00227605, 0.00390619, 0.00392157, ..., 0.00064591, 0.00038447,\n",
      "        0.00178393],\n",
      "       [0.00201461, 0.00390619, 0.00392157, ..., 0.00052288, 0.00033833,\n",
      "        0.00129181],\n",
      "       [0.00287582, 0.00390619, 0.00392157, ..., 0.00039985, 0.00026144,\n",
      "        0.00090734]], dtype=float32), array([[ 5.08244134e-03, -1.53074265e-02,  1.58821733e-02, ...,\n",
      "         3.73165409e-03, -2.72568195e-02, -6.49964829e-03],\n",
      "       [-1.36292961e-02, -6.50232329e-03, -9.93316703e-03, ...,\n",
      "        -5.25751777e-03,  3.07211907e-03, -9.15214526e-05],\n",
      "       [-3.07195255e-03,  6.83355568e-05, -1.30766256e-02, ...,\n",
      "         6.40852554e-03, -1.48317226e-02, -6.60479513e-03],\n",
      "       ...,\n",
      "       [-1.52477735e-03,  4.55562495e-03,  5.53566344e-04, ...,\n",
      "        -2.32920726e-03, -1.38621122e-03,  4.51630173e-03],\n",
      "       [ 7.57724029e-03,  1.10827418e-02, -4.93071454e-03, ...,\n",
      "        -2.92289454e-04, -4.61484823e-03, -7.34418897e-03],\n",
      "       [ 2.65864053e-03,  4.40117493e-03, -2.65170042e-03, ...,\n",
      "        -1.05270868e-03, -1.61137987e-03,  5.63379041e-03]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]))\n"
     ]
    }
   ],
   "source": [
    "W1 = parameters['W1']  \n",
    "b1 = parameters['b1'] \n",
    "A0 = train_x_orig.reshape(train_x_orig.shape[0], -1).T / 255.0 \n",
    "Z1, cache1 = linear_forward(A0, W1 , b1)\n",
    "print(\"Z1\", Z1)\n",
    "print(\"cache1\", cache1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b48caec8-e52f-4325-a379-16f052b0bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of sigmoid(Z), same shape as Z\n",
    "    cache -- Z, stored for backward pass\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0ead1b8-9b68-446e-8a96-22b9a418d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implements the relu activation.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of relu(Z), same shape as Z\n",
    "    cache -- Z, stored for backward pass\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec4a57e9-f89a-4f2b-9b77-25318c88374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "       \n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "       \n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache =  relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9c89efd-0265-4f35-aa97-c9b3faee51a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.00046374, 0.00256457, 0.00220269, ..., 0.00112307, 0.        ,\n",
       "         0.00065474],\n",
       "        ...,\n",
       "        [0.00019071, 0.        , 0.        , ..., 0.00106858, 0.00249432,\n",
       "         0.00019529],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.00220308, 0.00487628, 0.00637429, ..., 0.0035784 , 0.00261202,\n",
       "         0.00259312]]),\n",
       " ((array([[0.00226067, 0.00390619, 0.00392157, ..., 0.00219915, 0.00252211,\n",
       "           0.00387543],\n",
       "          [0.00215302, 0.00390619, 0.00392157, ..., 0.00196847, 0.00126105,\n",
       "           0.00387543],\n",
       "          [0.00284506, 0.00390619, 0.00392157, ..., 0.00152249, 0.00161476,\n",
       "           0.00384468],\n",
       "          ...,\n",
       "          [0.00227605, 0.00390619, 0.00392157, ..., 0.00064591, 0.00038447,\n",
       "           0.00178393],\n",
       "          [0.00201461, 0.00390619, 0.00392157, ..., 0.00052288, 0.00033833,\n",
       "           0.00129181],\n",
       "          [0.00287582, 0.00390619, 0.00392157, ..., 0.00039985, 0.00026144,\n",
       "           0.00090734]], dtype=float32),\n",
       "   array([[ 5.08244134e-03, -1.53074265e-02,  1.58821733e-02, ...,\n",
       "            3.73165409e-03, -2.72568195e-02, -6.49964829e-03],\n",
       "          [-1.36292961e-02, -6.50232329e-03, -9.93316703e-03, ...,\n",
       "           -5.25751777e-03,  3.07211907e-03, -9.15214526e-05],\n",
       "          [-3.07195255e-03,  6.83355568e-05, -1.30766256e-02, ...,\n",
       "            6.40852554e-03, -1.48317226e-02, -6.60479513e-03],\n",
       "          ...,\n",
       "          [-1.52477735e-03,  4.55562495e-03,  5.53566344e-04, ...,\n",
       "           -2.32920726e-03, -1.38621122e-03,  4.51630173e-03],\n",
       "          [ 7.57724029e-03,  1.10827418e-02, -4.93071454e-03, ...,\n",
       "           -2.92289454e-04, -4.61484823e-03, -7.34418897e-03],\n",
       "          [ 2.65864053e-03,  4.40117493e-03, -2.65170042e-03, ...,\n",
       "           -1.05270868e-03, -1.61137987e-03,  5.63379041e-03]]),\n",
       "   array([[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]])),\n",
       "  array([[-0.00450019, -0.00959911, -0.00836246, ..., -0.00551261,\n",
       "          -0.00599057, -0.00764693],\n",
       "         [-0.00352297, -0.00489753, -0.00658281, ..., -0.00412288,\n",
       "          -0.00497707, -0.00553568],\n",
       "         [ 0.00046374,  0.00256457,  0.00220269, ...,  0.00112307,\n",
       "          -0.00048832,  0.00065474],\n",
       "         ...,\n",
       "         [ 0.00019071, -0.00189658, -0.00299062, ...,  0.00106858,\n",
       "           0.00249432,  0.00019529],\n",
       "         [-0.00184841, -0.00303199, -0.00165671, ..., -0.00107128,\n",
       "          -0.00158586, -0.00020609],\n",
       "         [ 0.00220308,  0.00487628,  0.00637429, ...,  0.0035784 ,\n",
       "           0.00261202,  0.00259312]])))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_activation_forward(A0, W1, b1, \"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7174f6e-0165-4ad6-be52-dd155fdfab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- activation value from the output (last) layer\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    # The for loop starts at 1 because layer 0 is the input\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        \n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n",
    "        \n",
    "    caches.append(cache)\n",
    "          \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4529701-8eb4-49a5-92c8-c82dda29bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In our case X = A0.shape\n",
    "AL , caches = L_model_forward(A0, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba52ccef-a48e-443f-8f1c-77c7d129ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined \n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-face , 1 if face), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (-1/m) * np.sum((Y * np.log(AL)) + ((1-Y) * np.log(1-AL)))    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d9d46fb-c9a5-4f4e-b025-49df6ab3eb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931474283196609\n"
     ]
    }
   ],
   "source": [
    "Y = train_y.reshape(1, -1) #to make sure the size is consistent with AL\n",
    "\n",
    "cost = compute_cost(AL, Y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf80417c-4815-4d77-97e1-b9d53fa98ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m) * np.dot(dZ ,A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ) \n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcc4483b-1850-412c-a36e-d579871b4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a ReLU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    activation_cache -- 'Z' stored during forward propagation\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA, copy=True)  # just to avoid modifying dA directly\n",
    "\n",
    "    # Zero out gradients where Z was negative\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f453a844-e76d-42f0-abfd-717764cb65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, activation_cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a sigmoid unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    activation_cache -- 'Z' stored during forward propagation\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    # Compute the sigmoid of Z\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d47ef5eb-b2fb-45d2-9a4c-544b433ba19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "      \n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fa5984e-fbcf-4513-8e63-31aa59fd42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    " \n",
    "    current_cache = caches[L - 1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "    grads[\"dA\" + str(L - 1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "      \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb9d47e8-ca24-4feb-bd3a-77563dca6f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        \n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c145583-e0e9-47eb-a47c-d8dd7d9cfb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "   \n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                        \n",
    "        # Print the cost every 100 iterations and for the last iteration\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42371cc5-1cee-4286-8149-b26625f4492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the labels for a dataset X using learned parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of size (n_x, number of examples)\n",
    "    Y -- true labels of size (1, number of examples)\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2  # number of layers\n",
    "    p = np.zeros((1, m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    AL, _ = L_model_forward(X, parameters)\n",
    "    \n",
    "    # Convert probabilities to 0/1 predictions\n",
    "    for i in range(AL.shape[1]):\n",
    "        if AL[0, i] > 0.5:\n",
    "            p[0, i] = 1\n",
    "        else:\n",
    "            p[0, i] = 0\n",
    "    \n",
    "    print(\"Accuracy: \" + str(np.sum((p == Y)/m)))\n",
    "    \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88bf2fe1-2ee5-41da-8dbc-a86c0c5941b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6931472679575611\n",
      "Cost after iteration 100: 0.6876399102390719\n",
      "Cost after iteration 200: 0.6838561323205153\n",
      "Cost after iteration 300: 0.6812543740080552\n",
      "Cost after iteration 400: 0.6794634231402918\n",
      "Cost after iteration 500: 0.67822910199825\n",
      "Cost after iteration 600: 0.6773773963076135\n",
      "Cost after iteration 700: 0.6767890167453664\n",
      "Cost after iteration 800: 0.67638215574066\n",
      "Cost after iteration 900: 0.6761005661164011\n",
      "Cost after iteration 1000: 0.6759055318305845\n",
      "Cost after iteration 1100: 0.6757703482698231\n",
      "Cost after iteration 1200: 0.6756765914632535\n",
      "Cost after iteration 1300: 0.675611531912561\n",
      "Cost after iteration 1400: 0.6755663654322117\n",
      "Cost after iteration 1500: 0.675534998396586\n",
      "Cost after iteration 1600: 0.6755132045667356\n",
      "Cost after iteration 1700: 0.6754980590860971\n",
      "Cost after iteration 1800: 0.6754875304253517\n",
      "Cost after iteration 1900: 0.675480209751624\n",
      "Cost after iteration 2000: 0.6754751175342015\n",
      "Cost after iteration 2100: 0.6754715738623859\n",
      "Cost after iteration 2200: 0.6754691053318589\n",
      "Cost after iteration 2300: 0.6754673864692151\n",
      "Cost after iteration 2400: 0.675466187852675\n",
      "Cost after iteration 2499: 0.6754653579525982\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [64*64*3, 64, 32, 1]\n",
    "A0 = train_x_orig.reshape(train_x_orig.shape[0], -1).T / 255.0  # shape (12288, 224)\n",
    "Y0 = train_y.reshape(1, train_y.shape[0])  # shape (1, 224)\n",
    "parameters, costs = L_layer_model(A0, Y0, layer_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd3ef7b7-2af6-41ee-887a-b951afe4ef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5937499999999998\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(A0, Y0, parameters)\n",
    "print(pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f219de-e89c-4a3f-a496-398dd12e2829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
